{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End TigerGraph GraphRag for Document Question Answering\n",
    "\n",
    "This notebook demostrates how to use TigerGraph Graph-RAG (currently in Beta), an AI assistant for your TigerGraph databases. TigerGraph Graph-RAG enables you to ask questions in natural language about your document data stored in TigerGraph and get answers in a human-readable format. GraphRAG is a graph-based retrieval-augmented generation approach that is used to answer questions about the document data stored in TigerGraph. TigerGraph Graph-RAG is built to help users get started with GraphRAG and to provide a seamless experience for users to interact with their document data within TigerGraph.\n",
    "\n",
    "## Setup Environment\n",
    "\n",
    "\n",
    "* Follow [Docker setup ](https://github.com/tigergraph/ecosys/blob/master/demos/guru_scripts/docker/README.md) to set up your docker Environment.\n",
    "* Please follow (Overview of installing Docker Compose)[https://docs.docker.com/compose/install/] to install Docker Compose for your platform accordingly.\n",
    "\n",
    "\n",
    "#### TigerGraph Docker Image\n",
    "\n",
    "To use TigerGraph Community Edition without a license key, download the corresponding docker image from https://dl.tigergraph.com/ and load to Docker:\n",
    "```\n",
    "docker load -i ./tigergraph-4.2.0-community-docker-image.tar.gz\n",
    "docker images\n",
    "```\n",
    "\n",
    "You should be able to find `tigergraph/community:4.2.0` in the image list.\n",
    "\n",
    "#### Graph-RAG Docker Images\n",
    "\n",
    "The following images are also needed for TigerGraph Graph-RAG. Docker Compose will automatically download them, but you can download them manually if preferred:\n",
    "\n",
    "```\n",
    "docker pull <image_name>\n",
    "\n",
    "tigergraph/graphrag:latest\n",
    "tigergraph/ecc:latest\n",
    "tigergraph/chat-history:latest\n",
    "tigergraph/graphrag-ui:latest\n",
    "nginx:latest\n",
    "```\n",
    "\n",
    "### Deploy Graph-RAG with Docker Compose\n",
    "#### Get docker-compose file\n",
    "Download the [docker-compose.yml](https://raw.githubusercontent.com/tigergraph/ecosys/refs/heads/master/tutorials/graphrag/docker-compose.yml) file directly\n",
    "\n",
    "The Docker Compose file contains all dependencies for Graph-RAG including a TigerGraph database. If you want to use a separate TigerGraph instance, you can comment out the `tigergraph` section from the docker compose file and restart all services. However, please follow the instructions below to make sure your standalone TigerGraph server is accessible from other Graph-RAG containers.\n",
    "\n",
    "#### Set up configurations\n",
    "\n",
    "Next, download the following configuration files and put them in a `configs` subdirectory of the directory contains the Docker Compose file:\n",
    "* [configs/db_config.json](https://raw.githubusercontent.com/tigergraph/ecosys/refs/heads/master/tutorials/graphrag/configs/db_config.json)\n",
    "* [configs/llm_config.json](https://raw.githubusercontent.com/tigergraph/ecosys/refs/heads/master/tutorials/graphrag/configs/llm_config.json)\n",
    "* [configs/chat_config.json](https://raw.githubusercontent.com/tigergraph/ecosys/refs/heads/master/tutorials/graphrag/configs/chat_config.json)\n",
    "* [configs/nginx.conf](https://raw.githubusercontent.com/tigergraph/ecosys/refs/heads/master/tutorials/graphrag/configs/nginx.conf)\n",
    "\n",
    "#### Adjust configurations\n",
    "\n",
    "Edit `configs/llm_config.json` and replace `<YOUR_OPENAI_API_KEY>` to your own OPENAI_API_KEY. \n",
    " \n",
    "> If desired, you can also change the model to be used for the embedding service and completion service to your preferred models to adjust the output from the LLM service.\n",
    "\n",
    "#### Start all services\n",
    "\n",
    "Now, simply run `docker compose up -d` and wait for all the services to start.\n",
    "\n",
    "## Build GraphRAG From Scratch\n",
    "\n",
    "If you want to experience the whole process of Graph-RAG, you can build the GraphRAG from scratch. However, please review the LLM model and service setting carefully because it will cost some money to re-generate embedding and data structure for the raw data.\n",
    "\n",
    "#### Step 1: Database Connection Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyTigerGraph import TigerGraphConnection\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# We first create a connection to the database\n",
    "host = \"http://192.168.11.12\"\n",
    "username = os.getenv(\"USERNAME\", \"tigergraph\")\n",
    "password = os.getenv(\"PASS\", \"tigergraph\")\n",
    "conn = TigerGraphConnection(\n",
    "    host=host,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    gsPort=\"14240\",\n",
    "    restppPort=\"14240\",\n",
    "    graphname = \"TigerGraphRAG\"\n",
    ")\n",
    "\n",
    "# And then add GraphRAG's address to the connection. This address\n",
    "# is the host's address where the GraphRAG container is running.\n",
    "conn.ai.configureGraphRAGHost(f\"{host}:8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Graph and Ingest Data\n",
    "\n",
    "We provide utilities to setup your TigerGraph database with a schema and load your desired documents. In this example, we are utilizing the TigerGraph documentation as our dataset. The documents are processed into a JSONL file of the following format:\n",
    "\n",
    "```json\n",
    "{\"url\": \"some_url_here\", \"content\": \"Text of the document\"}\n",
    "```\n",
    "\n",
    "The following code block will create a graph called `TigerGraphRAG` and load the documents into the graph. The schema that is created looks like this:\n",
    "\n",
    "![graphrag_schema](./pictures/GraphRAGSchema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.gsql(f\"\"\"CREATE GRAPH {conn.graphname}()\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get connection token if authentication is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to authenticate the connection\n",
    "conn.getToken()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SuportAI schema and install related queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.ai.initializeSupportAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DocumentIngest for local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = conn.ai.createDocumentIngest(\n",
    "    data_source=\"local\",\n",
    "    data_source_config={\"data_path\": \"./data/tg_tutorials.jsonl\"},\n",
    "    loader_config={\"doc_id_field\": \"doc_id\", \"content_field\": \"content\", \"doc_type\": \"markdown\"},\n",
    "    file_format=\"json\",\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run DocumentIngest to load documents to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.ai.runDocumentIngest(res[\"load_job_id\"], res[\"data_source_id\"], res[\"data_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, create and run DocumentIngest for data files on Cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access = \"\"\n",
    "sec = \"\"\n",
    "res = conn.ai.createDocumentIngest(\n",
    "    data_source=\"s3\",\n",
    "    data_source_config={\"aws_access_key\": access, \"aws_secret_key\": sec},\n",
    "    loader_config={\"doc_id_field\": \"url\", \"content_field\": \"content\", \"doc_type\": \"\"},\n",
    "    file_format=\"json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.ai.runDocumentIngest(res[\"load_job_id\"], res[\"data_source_id\"], \"s3://tg-documentation/pytg_current/pytg_current.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Knowledge Graph from the documents loaded\n",
    "\n",
    "Constructing the full knowledge graph with semantics, context, and structure: we trigger an end-to-end pipeline that performs llm integration operations:context, and structure: including document chunking, embedding, upserting, extraction, and community detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.ai.forceConsistencyUpdate(\"graphrag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Document Search Methods\n",
    "\n",
    "TigerGraph GraphRAG offers hybrid vector and graph-based methods for searching documents within the graph, including:\n",
    "\n",
    "- **Hybrid**: This method uses a combination of vector search and graph traversal to find the most relevant information to the query. It uses the similarity algorithm to search the embeddings of documents, document chunks, entities, and relationships. These results serve as the starting point for the graph traversal. The graph traversal is used to find the most relevant information to the query.\n",
    "\n",
    "\n",
    "- **GraphRAG (Community Search)**: This method enhances retrieval by leveraging graph traversal and community detection. It starts from top-k similar communities and performs a graph traversal across relevant relationships to identify communities of related chunks. The traversal is guided by connection patterns in the graph rather than just semantic similarity, enabling richer and more coherent context retrieval. GraphRAG is especially effective in complex knowledge graphs where multi-hop reasoning or structural connections are important.\n",
    "\n",
    "TigerGraph GraphRAG provides a way to generate the response to the user's query using a LLM, based on the search results from the methods above. You can compare the responses generated by the LLM for each of the search methods to see which one is the most relevant to the user's query. In this example, we can see that the Hybrid method generates the most relevant response to the user's query.\n",
    "\n",
    "Now, open [http://{your-server-ip}:80](http://192.168.11.12:80) to access and try the GraphRag system using UI. \n",
    "\n",
    "In addition to the UI, APIs are available to programmatically generate responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I connect to a TigerGraph database using Python?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer question using GraphRAG (Community Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = conn.ai.answerQuestion(query,\n",
    "                        method=\"graphrag\",\n",
    "                        method_parameters={\"community_level\": 2, \"top_k\": 3, \"verbose\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp[\"retrieved\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check verbose info for more details if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(resp[\"verbose\"], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer question using Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = conn.ai.answerQuestion(query,\n",
    "                        method=\"hybrid\",\n",
    "                        method_parameters = {\"indices\": [\"Document\", \"DocumentChunk\", \"Entity\", \"Relationship\"],\n",
    "                                             \"top_k\": 5,\n",
    "                                             \"num_hops\": 2,\n",
    "                                             \"num_seen_min\": 3,\n",
    "                                             \"verbose\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp[\"retrieved\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp[\"retrieved\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
